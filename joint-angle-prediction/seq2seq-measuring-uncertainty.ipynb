{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model for Joint Angle Prediction and Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff2f4f98c70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "import math, random, time, os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from common.data_utils import *\n",
    "from common.models import *\n",
    "from common.training_utils import *\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../csv-files')\n",
    "FILENAMES = ['IMU_S1_2017_04_02_1.csv','IMU_S2_2017_04_10_1a_a1.csv',\n",
    "             'IMU_S1_2017_04_11_03a_a_1.csv']\n",
    "\n",
    "joint_requests = {'Joint' : ['jL5S1','jL4L3','jL1T12','jT9T8','jT1C7','jC1Head',\n",
    "                             'jRightC7Shoulder','jRightShoulder','jRightElbow','jRightWrist',\n",
    "                             'jLeftC7Shoulder','jLeftShoulder','jLeftElbow','jLeftWrist',\n",
    "                             'jRightHip','jRightKnee','jRightAnkle','jRightBallFoot',\n",
    "                             'jLeftHip','jLeftKnee','jLeftAnkle','jLeftBallFoot']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with reading files\n"
     ]
    }
   ],
   "source": [
    "positions = None\n",
    "joint_angles = None\n",
    "seq_length = 120\n",
    "files = ['IMU_S1_2017_04_02_1.csv','IMU_S2_2017_04_10_1a_a1.csv',\n",
    "         'IMU_S1_2017_04_11_03a_a_1.csv']\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    \n",
    "    joint_target_columns = request_indices(joint_requests)\n",
    "    \n",
    "    print(\"Index: \" + str(idx + 1), end='\\r')\n",
    "    \n",
    "    joint_angles_temp = np.loadtxt(DATA_PATH / file, delimiter=',', usecols=joint_target_columns)\n",
    "    \n",
    "    discard_remainder(joint_angles_temp, seq_length)\n",
    "    \n",
    "    joint_angles_temp *= (np.pi/180.)\n",
    "    \n",
    "    if idx == 0:\n",
    "        joint_angles = joint_angles_temp\n",
    "    else:\n",
    "        joint_angles = np.concatenate((joint_angles, joint_angles_temp), axis=0)\n",
    "print(\"Done with reading files\")\n",
    "\n",
    "joint_angles = preprocessing.normalize(joint_angles, axis=0)\n",
    "\n",
    "joint_angles = reshape_to_sequences(joint_angles, seq_length=seq_length) \n",
    "\n",
    "data = joint_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_target_data = split_sequences(data, seq_length=seq_length)\n",
    "\n",
    "batch_size = 32\n",
    "encoder_input_data = discard_remainder(encoder_input_data, batch_size)\n",
    "decoder_target_data = discard_remainder(decoder_target_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using Dataset\n",
    "------------------------------\n",
    "\n",
    "PyTorch has an abstract Dataset class.  A Dataset can be anything that has\n",
    "a ``__len__`` function (called by Python's standard ``len`` function) and\n",
    "a ``__getitem__`` function as a way of indexing into it.\n",
    "__[This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)__\n",
    "walks through a nice example of creating a custom ``FacialLandmarkDataset`` class\n",
    "as a subclass of ``Dataset``.\n",
    "\n",
    "PyTorch's __[TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset)__\n",
    "is a Dataset wrapping tensors. **By defining a length and way of indexing,\n",
    "this also gives us a way to iterate, index, and slice along the first\n",
    "dimension of a tensor.** This will make it easier to access both the\n",
    "independent and dependent variables in the same line as we train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = torch.tensor(encoder_input_data).to(device)\n",
    "decoder_target_data = torch.tensor(decoder_target_data).to(device)\n",
    "\n",
    "dataset = TensorDataset(encoder_input_data, decoder_target_data)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Dropout\n",
    "\n",
    "I used variational dropout that I found __[here](https://discuss.pytorch.org/t/variational-dropout/23030/4)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, alpha=1.0, dim=None):\n",
    "        super(VariationalDropout, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.max_alpha = alpha\n",
    "        # Initial alpha\n",
    "        log_alpha = (torch.ones(dim) * alpha).log()\n",
    "        self.log_alpha = nn.Parameter(log_alpha)\n",
    "        \n",
    "    def kl(self):\n",
    "        c1 = 1.16145124\n",
    "        c2 = -1.50204118\n",
    "        c3 = 0.58629921\n",
    "        \n",
    "        alpha = self.log_alpha.exp()\n",
    "        \n",
    "        negative_kl = 0.5 * self.log_alpha + c1 * alpha + c2 * alpha**2 + c3 * alpha**3\n",
    "        \n",
    "        kl = -negative_kl\n",
    "        \n",
    "        return kl.mean()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Sample noise   e ~ N(1, alpha)\n",
    "        Multiply noise h = h_ * e\n",
    "        \"\"\"\n",
    "        if self.train():\n",
    "            # N(0,1)\n",
    "            epsilon = Tensor(torch.randn(x.size()))\n",
    "            if x.is_cuda:\n",
    "                epsilon = epsilon.cuda()\n",
    "\n",
    "            # Clip alpha\n",
    "            self.log_alpha.data = torch.clamp(self.log_alpha.data, max=self.max_alpha)\n",
    "            alpha = self.log_alpha.exp()\n",
    "\n",
    "            # N(1, alpha)\n",
    "            epsilon = epsilon * alpha\n",
    "\n",
    "            return x * epsilon\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.dropout = VariationalDropout(alpha=0.05, dim=(1, batch_size, hidden_size))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.dropout(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.tanh(self.out(output))\n",
    "        output = self.dropout(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(num_features, hidden_size=64, lr=0.001, bs=batch_size):\n",
    "    encoder = EncoderRNN(num_features, hidden_size, bs).to(device)\n",
    "    return encoder, optim.Adam(encoder.parameters(), lr=lr)\n",
    "\n",
    "def get_decoder(num_features, hidden_size=64, lr=0.001, bs=batch_size):\n",
    "    decoder = DecoderRNN(num_features, hidden_size, num_features, bs).to(device)\n",
    "    return decoder, optim.Adam(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, models, opts, criterion, teacher_forcing_ratio):\n",
    "    loss = 0\n",
    "    \n",
    "    encoder, decoder = models\n",
    "    \n",
    "    if opts is not None:\n",
    "        encoder_opt, decoder_opt = opts \n",
    "    \n",
    "    input_batch, target_batch = data\n",
    "    \n",
    "    seq_length = input_batch.shape[1]\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    for t in range(seq_length):\n",
    "        input = input_batch[:, t, :].unsqueeze(0).float()\n",
    "        _, encoder_hidden = encoder(input, encoder_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.ones_like(target_batch[:, 0, :]).unsqueeze(0).float()\n",
    "    EOS = torch.zeros_like(target_batch[:, 0, :]).unsqueeze(0).float()\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        target = target_batch[:, t, :].unsqueeze(0).float()\n",
    "        loss += criterion(decoder_output, target)\n",
    "        if torch.all(torch.eq(decoder_output, EOS)):\n",
    "            break\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target\n",
    "        else:\n",
    "            decoder_input = decoder_output.detach()\n",
    "\n",
    "    if opts is not None:\n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_opt.step()\n",
    "        encoder_opt.zero_grad()\n",
    "\n",
    "        decoder_opt.step()\n",
    "        decoder_opt.zero_grad()\n",
    "    \n",
    "    return loss.item() / seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(models, opts, epochs, dataloaders, criterion, teacher_forcing_ratio):\n",
    "    \n",
    "    train_dataloader, val_dataloader = dataloaders\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for index, data in enumerate(train_dataloader, 0):\n",
    "            loss = train(data, models, \n",
    "                         opts, criterion,\n",
    "                         teacher_forcing_ratio) \n",
    "            print(\"Iteration: \" + str(index + 1), \"Loss: \" + str(loss), end=\"\\r\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # calculating loss for validation\n",
    "            losses = [train(data, models, None, criterion, teacher_forcing_ratio) \n",
    "                      for _, data in enumerate(val_dataloader, 0)]\n",
    "        val_loss = np.sum(losses) / len(losses)\n",
    "        print()\n",
    "        print(\"Epoch: \" + str(epoch + 1), \"Training Loss: \" + str(loss), \"Val Loss: \" + str(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 176 Loss: 0.07834654649098714\n",
      "Epoch: 1 Training Loss: 0.07834654649098714 Val Loss: 0.08326163996349682\n",
      "Iteration: 176 Loss: 0.07959001064300537\n",
      "Epoch: 2 Training Loss: 0.07959001064300537 Val Loss: 0.07075883402968898\n",
      "Iteration: 176 Loss: 0.066588171323140464\n",
      "Epoch: 3 Training Loss: 0.06658817132314046 Val Loss: 0.06556793812549476\n",
      "Iteration: 176 Loss: 0.065823249022165945\n",
      "Epoch: 4 Training Loss: 0.06582324902216594 Val Loss: 0.06284450292587282\n",
      "Iteration: 176 Loss: 0.063518480459849045\n",
      "Epoch: 5 Training Loss: 0.06351848045984904 Val Loss: 0.060856935562509484\n",
      "Iteration: 176 Loss: 0.054184639453887944\n",
      "Epoch: 6 Training Loss: 0.05418463945388794 Val Loss: 0.0597373821518638\n",
      "Iteration: 176 Loss: 0.051368101437886555\n",
      "Epoch: 7 Training Loss: 0.05136810143788655 Val Loss: 0.05896982776396202\n",
      "Iteration: 176 Loss: 0.061881140867869066\n",
      "Epoch: 8 Training Loss: 0.06188114086786906 Val Loss: 0.05859747032324472\n",
      "Iteration: 176 Loss: 0.054642355442047124\n",
      "Epoch: 9 Training Loss: 0.05464235544204712 Val Loss: 0.057499820535833183\n",
      "Iteration: 176 Loss: 0.054480572541554774\n",
      "Epoch: 10 Training Loss: 0.05448057254155477 Val Loss: 0.0569634429433129\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "encoder, encoder_opt = get_encoder(66)\n",
    "decoder, decoder_opt = get_decoder(66)\n",
    "teacher_forcing_ratio = 0.0\n",
    "\n",
    "models = (encoder, decoder)\n",
    "opts = (encoder_opt, decoder_opt)\n",
    "\n",
    "dataloaders = (train_dataloader, val_dataloader)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "loss = 0\n",
    "\n",
    "fit(models, opts, epochs, dataloaders, criterion, teacher_forcing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Model Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_uncertainty(data, models, criterion, teacher_forcing_ratio):\n",
    "    B = 10\n",
    "    loss = 0\n",
    "    \n",
    "    encoder, decoder = models\n",
    "    \n",
    "    input_batch, target_batch = data\n",
    "    \n",
    "    batch_size = input_batch.shape[0]\n",
    "    seq_length = input_batch.shape[1]\n",
    "    num_features = input_batch.shape[2]\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    y_mc = torch.zeros(seq_length, batch_size, num_features).to(device)\n",
    "    y_b = torch.zeros(B, seq_length, batch_size, num_features).to(device)\n",
    "    eta_squared = torch.zeros(seq_length, batch_size, num_features).to(device)\n",
    "\n",
    "    for b in range(B):\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            input = input_batch[:, t, :].unsqueeze(0).float() # input of size 1 x batch_size x num_features\n",
    "            _, encoder_hidden = encoder(input, encoder_hidden)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.ones_like(target_batch[:, 0, :]).unsqueeze(0).float()\n",
    "        EOS = torch.zeros_like(target_batch[:, 0, :]).unsqueeze(0).float()\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            y_mc[t, :, :] += decoder_output.squeeze(0)\n",
    "            y_b[b, t, :, :] = decoder_output\n",
    "            \n",
    "            target = target_batch[:, t, :].unsqueeze(0).float()\n",
    "            loss += criterion(decoder_output, target)\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = target\n",
    "            else:\n",
    "                if torch.all(torch.eq(decoder_output, EOS)):\n",
    "                    break\n",
    "                decoder_input = decoder_output.detach()\n",
    "    \n",
    "    y_mc /= B\n",
    "    \n",
    "    for b in range(B):\n",
    "        y_pred = y_b[b]\n",
    "        eta_squared += torch.abs(y_pred - y_mc)\n",
    "    \n",
    "    eta = torch.sqrt(eta_squared / B)\n",
    "    \n",
    "    return loss.item() / (seq_length * B), eta, y_mc\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_quantification(models, dataloaders, criterion, teacher_forcing_ratio):\n",
    "    _, val_dataloader = dataloaders\n",
    "    \n",
    "    val_loss = 0\n",
    "    etas = torch.zeros(len(val_dataloader), 120, 66)\n",
    "    with torch.no_grad():\n",
    "        # calculating eta\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            loss, eta, y_mc = model_uncertainty(data, models, criterion, teacher_forcing_ratio)\n",
    "            val_loss += loss\n",
    "            etas[i, :, :] = torch.mean(eta, 1)\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        etas_from_features = torch.softmax(torch.max(torch.mean(eta, 0)), dim=0)\n",
    "    return val_loss, etas_from_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-c71018176db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metas_from_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muncertainty_quantification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metas_from_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metas_from_featuers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.0\n",
    "\n",
    "losses, etas_from_features = uncertainty_quantification(models, dataloaders, criterion, teacher_forcing_ratio)\n",
    "losses, etas_from_features = etas_from_featuers.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 120, 66)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etas_from_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "descending_most_uncertain = np.argsort(etas_from_features)[::-1]\n",
    "\n",
    "joint_rotation_indices = [(i // 3, i % 3) for i in descending_most_uncertain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jLeftElbow', 'Abduction/adduction'),\n",
       " ('jLeftElbow', 'Internal/external rotation'),\n",
       " ('jRightElbow', 'Abduction/adduction'),\n",
       " ('jRightElbow', 'Internal/external rotation'),\n",
       " ('jLeftHip', 'Abduction/adduction'),\n",
       " ('jRightElbow', 'Flexion/extraction'),\n",
       " ('jLeftElbow', 'Flexion/extraction'),\n",
       " ('jRightWrist', 'Internal/external rotation'),\n",
       " ('jLeftKnee', 'Abduction/adduction'),\n",
       " ('jRightShoulder', 'Internal/external rotation'),\n",
       " ('jRightWrist', 'Flexion/extraction'),\n",
       " ('jLeftShoulder', 'Internal/external rotation'),\n",
       " ('jLeftHip', 'Internal/external rotation'),\n",
       " ('jRightHip', 'Internal/external rotation'),\n",
       " ('jRightShoulder', 'Abduction/adduction'),\n",
       " ('jC1Head', 'Abduction/adduction'),\n",
       " ('jLeftShoulder', 'Abduction/adduction'),\n",
       " ('jLeftShoulder', 'Flexion/extraction'),\n",
       " ('jRightShoulder', 'Flexion/extraction'),\n",
       " ('jLeftKnee', 'Internal/external rotation'),\n",
       " ('jLeftWrist', 'Internal/external rotation'),\n",
       " ('jT1C7', 'Internal/external rotation'),\n",
       " ('jRightWrist', 'Abduction/adduction'),\n",
       " ('jRightKnee', 'Internal/external rotation'),\n",
       " ('jLeftAnkle', 'Internal/external rotation'),\n",
       " ('jLeftBallFoot', 'Internal/external rotation'),\n",
       " ('jLeftWrist', 'Abduction/adduction'),\n",
       " ('jRightAnkle', 'Internal/external rotation'),\n",
       " ('jRightHip', 'Abduction/adduction'),\n",
       " ('jLeftWrist', 'Flexion/extraction'),\n",
       " ('jRightAnkle', 'Abduction/adduction'),\n",
       " ('jLeftHip', 'Flexion/extraction'),\n",
       " ('jRightBallFoot', 'Internal/external rotation'),\n",
       " ('jLeftC7Shoulder', 'Abduction/adduction'),\n",
       " ('jLeftC7Shoulder', 'Flexion/extraction'),\n",
       " ('jC1Head', 'Flexion/extraction'),\n",
       " ('jLeftAnkle', 'Abduction/adduction'),\n",
       " ('jC1Head', 'Internal/external rotation'),\n",
       " ('jRightC7Shoulder', 'Abduction/adduction'),\n",
       " ('jLeftC7Shoulder', 'Internal/external rotation'),\n",
       " ('jLeftKnee', 'Flexion/extraction'),\n",
       " ('jRightHip', 'Flexion/extraction'),\n",
       " ('jRightKnee', 'Abduction/adduction'),\n",
       " ('jT1C7', 'Abduction/adduction'),\n",
       " ('jRightC7Shoulder', 'Flexion/extraction'),\n",
       " ('jL5S1', 'Abduction/adduction'),\n",
       " ('jRightC7Shoulder', 'Internal/external rotation'),\n",
       " ('jT1C7', 'Flexion/extraction'),\n",
       " ('jL5S1', 'Internal/external rotation'),\n",
       " ('jRightAnkle', 'Flexion/extraction'),\n",
       " ('jLeftAnkle', 'Flexion/extraction'),\n",
       " ('jRightKnee', 'Flexion/extraction'),\n",
       " ('jL5S1', 'Flexion/extraction'),\n",
       " ('jL4L3', 'Internal/external rotation'),\n",
       " ('jL1T12', 'Internal/external rotation'),\n",
       " ('jRightBallFoot', 'Abduction/adduction'),\n",
       " ('jT9T8', 'Internal/external rotation'),\n",
       " ('jL1T12', 'Abduction/adduction'),\n",
       " ('jL4L3', 'Abduction/adduction'),\n",
       " ('jLeftBallFoot', 'Abduction/adduction'),\n",
       " ('jT9T8', 'Abduction/adduction'),\n",
       " ('jL4L3', 'Flexion/extraction'),\n",
       " ('jL1T12', 'Flexion/extraction'),\n",
       " ('jRightBallFoot', 'Flexion/extraction'),\n",
       " ('jLeftBallFoot', 'Flexion/extraction'),\n",
       " ('jT9T8', 'Flexion/extraction')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_names = ['jL5S1','jL4L3','jL1T12','jT9T8','jT1C7','jC1Head',\n",
    "          'jRightC7Shoulder','jRightShoulder','jRightElbow','jRightWrist',\n",
    "          'jLeftC7Shoulder','jLeftShoulder','jLeftElbow','jLeftWrist',\n",
    "          'jRightHip','jRightKnee','jRightAnkle','jRightBallFoot',\n",
    "          'jLeftHip','jLeftKnee','jLeftAnkle','jLeftBallFoot']\n",
    "\n",
    "#Euler  angle  extractions  of  Z (flexion/extension), X (abduction/adduction), Y (internal/external rotation)\n",
    "angle_names = ['Flexion/extraction', 'Abduction/adduction', 'Internal/external rotation']\n",
    "\n",
    "indices = np.arange(len(joints))+1\n",
    "\n",
    "joint_indices = dict(zip(indices, joint_names))\n",
    "\n",
    "angle_indices = dict(zip([1, 2, 3], angle_names))\n",
    "\n",
    "joint_rotation_indices = [(joint_indices[i+1], angle_indices[j+1]) for i, j in joint_rotation_indices]\n",
    "\n",
    "joint_rotation_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Model Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(num_features, hidden_size=64, lr=0.001, bs=batch_size):\n",
    "    encoder = EncoderRNN(num_features, hidden_size, bs).to(device)\n",
    "    return encoder, optim.Adam(encoder.parameters(), lr=lr)\n",
    "\n",
    "def get_decoder(num_features, hidden_size=64, lr=0.001, bs=batch_size):\n",
    "    decoder = DecoderRNN(num_features, hidden_size, num_features, bs).to(device)\n",
    "    return decoder, optim.Adam(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_l1_loss(input, target, weights, reduction='weighted'):\n",
    "    if target.requires_grad:\n",
    "        ret = torch.abs(input - target)\n",
    "        if reduction != 'none':\n",
    "            ret = torch.dot(ret, weights) if reduction == 'weighted' else torch.mean(ret)\n",
    "    else:\n",
    "        expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
    "        ret = torch.abs(expanded_input - expanded_target)\n",
    "        if reduction != 'none':\n",
    "            ret = torch.dot(ret, weights) if reduction == 'weighted' else torch.mean(ret)\n",
    "    return ret\n",
    "\n",
    "class Weighted_L1Loss(torch.nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, reduction='weighted'):\n",
    "        super(Weighted_L1Loss, self).__init__(reduction)\n",
    "\n",
    "    def forward(self, input, target, weights):\n",
    "        return weighted_l1_loss(input, target, weights, reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, models, opts, criterion, teacher_forcing_ratio, weights):\n",
    "    loss = 0\n",
    "    \n",
    "    encoder, decoder = models\n",
    "    \n",
    "    if opts is not None:\n",
    "        encoder_opt, decoder_opt = opts \n",
    "    \n",
    "    input_batch, target_batch = data\n",
    "    \n",
    "    seq_length = input_batch.shape[1]\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    for t in range(seq_length):\n",
    "        input = input_batch[:, t, :].unsqueeze(0).float()\n",
    "        _, encoder_hidden = encoder(input, encoder_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.ones_like(target_batch[:, 0, :]).unsqueeze(0).float()\n",
    "    EOS = torch.zeros_like(target_batch[:, 0, :]).unsqueeze(0).float()\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        target = target_batch[:, t, :].unsqueeze(0).float()\n",
    "        if weights is None:\n",
    "            loss += criterion(decoder_output, target)\n",
    "        else:\n",
    "            loss += criterion(decoder_output, target, weights)\n",
    "            \n",
    "        if torch.all(torch.eq(decoder_output, EOS)):\n",
    "            break\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target\n",
    "        else:\n",
    "            decoder_input = decoder_output.detach()\n",
    "\n",
    "    if opts is not None:\n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_opt.step()\n",
    "        encoder_opt.zero_grad()\n",
    "\n",
    "        decoder_opt.step()\n",
    "        decoder_opt.zero_grad()\n",
    "    \n",
    "    return loss.item() / seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(models, opts, epochs, dataloaders, criteria, teacher_forcing_ratio, weights):\n",
    "    \n",
    "    train_dataloader, val_dataloader = dataloaders\n",
    "    Weighted_L1Loss, L1Loss = criteria\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for index, data in enumerate(train_dataloader, 0):\n",
    "            loss = train(data, models, \n",
    "                         opts, Weighted_L1Loss,\n",
    "                         teacher_forcing_ratio, weights) \n",
    "            print(\"Iteration: \" + str(index + 1), \"Loss: \" + str(loss), end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            # calculating loss for validation\n",
    "            losses = [train(data, models, None, L1Loss, teacher_forcing_ratio, None) \n",
    "                      for _, data in enumerate(val_dataloader, 0)]\n",
    "        val_loss = np.sum(losses) / len(losses)\n",
    "\n",
    "        _, eta_from_features = uncertainty_quantification(models, dataloaders, L1Loss, teacher_forcing_ratio)    \n",
    "        weights = eta_from_features\n",
    "        print()\n",
    "        print(\"Epoch: \" + str(epoch + 1), \"Training Loss: \" + str(loss), \"Val Loss: \" + str(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 176 Loss: 0.08187534014383951\n",
      "Epoch: 1 Training Loss: 0.08187534014383951 Val Loss: 0.08233602841695149\n",
      "Iteration: 176 Loss: 0.06941592693328857\n",
      "Epoch: 2 Training Loss: 0.06941592693328857 Val Loss: 0.06888229846954347\n",
      "Iteration: 176 Loss: 0.074510796864827484\n",
      "Epoch: 3 Training Loss: 0.07451079686482748 Val Loss: 0.06471657653649648\n",
      "Iteration: 176 Loss: 0.055969893932342534\n",
      "Epoch: 4 Training Loss: 0.05596989393234253 Val Loss: 0.062103370644829486\n",
      "Iteration: 176 Loss: 0.065044792493184414\n",
      "Epoch: 5 Training Loss: 0.06504479249318441 Val Loss: 0.06003641016555555\n",
      "Iteration: 176 Loss: 0.062653764088948574\n",
      "Epoch: 6 Training Loss: 0.06265376408894857 Val Loss: 0.05916433641404817\n",
      "Iteration: 176 Loss: 0.061070084571838384\n",
      "Epoch: 7 Training Loss: 0.06107008457183838 Val Loss: 0.058655395652308616\n",
      "Iteration: 176 Loss: 0.053784346580505374\n",
      "Epoch: 8 Training Loss: 0.05378434658050537 Val Loss: 0.05804782816858003\n",
      "Iteration: 176 Loss: 0.056990563869476325\n",
      "Epoch: 9 Training Loss: 0.05699056386947632 Val Loss: 0.05767647018938354\n",
      "Iteration: 176 Loss: 0.057655362288157146\n",
      "Epoch: 10 Training Loss: 0.057655362288157146 Val Loss: 0.05698584002075773\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "encoder, encoder_opt = get_encoder(66)\n",
    "decoder, decoder_opt = get_decoder(66)\n",
    "teacher_forcing_ratio = 0.0\n",
    "\n",
    "models = (encoder, decoder)\n",
    "opts = (encoder_opt, decoder_opt)\n",
    "weights = torch.ones(66) / 66.\n",
    "dataloaders = (train_dataloader, val_dataloader)\n",
    "\n",
    "criteria = (Weighted_L1Loss(), nn.L1Loss())\n",
    "loss = 0\n",
    "\n",
    "fit(models, opts, epochs, dataloaders, criteria, teacher_forcing_ratio, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
