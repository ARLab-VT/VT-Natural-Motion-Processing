{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference with our Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "from common.quaternion import quat_mul\n",
    "from common.data_utils import read_h5\n",
    "from common.skeleton import Skeleton\n",
    "from seq2seq.training_utils import get_encoder, get_attn_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in some data to test with\n",
    "\n",
    "Let's start by loading in data.\n",
    "\n",
    "We trained our model using \"normOrientation\" and \"normAcceleration\" from the T8 (sternum), both forearms, and both lower legs as input, so we'll read in that data. We'll also read in some data for \"normOrientation\" on the entire body because this is the output of our model. Finally, we'll read in data for the orientation of the pelvis. This is important because we want to rotate the \"normOrientation\" data back into it's original reference frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [\"T8\", \"RightForeArm\", \"RightLowerLeg\", \"LeftForeArm\", \"LeftLowerLeg\"]\n",
    "\n",
    "filepaths = glob.glob(\"../data/*.h5\")\n",
    "requests = {\"normOrientation\" : [\"all\"], \"orientation\" : [\"Pelvis\"]}\n",
    "dataset = read_h5(filepaths, requests)\n",
    "\n",
    "filename = filepaths[0].split(\"/\")[-1]\n",
    "fullBodyOrientations = torch.Tensor(dataset[filename][\"normOrientation\"]).double()\n",
    "root = torch.Tensor(dataset[filename]['orientation']).double()\n",
    "\n",
    "requests = {\"normOrientation\" : group, \"normAcceleration\" : group}\n",
    "dataset = read_h5(filepaths, requests)\n",
    "orientationInputs = torch.Tensor(dataset[filename][\"normOrientation\"]).double()\n",
    "accelerationInputs = torch.Tensor(dataset[filename][\"normAcceleration\"]).double()\n",
    "\n",
    "with h5py.File(\"../data/set-2/normalization.h5\", \"r\") as f:\n",
    "    mean, std_dev = torch.Tensor(f[\"mean\"]), torch.Tensor(f[\"std_dev\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the Seq2Seq Model\n",
    "\n",
    "We can now load in our Seq2Seq models (encoder and decoder).\n",
    "\n",
    "The models must have the same arguments used during training so that errors don't pop up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_feature_size = len(group)*4 + len(group)*3\n",
    "decoder_feature_size = 92\n",
    "hidden_size = 512\n",
    "attention = \"dot\"\n",
    "bidirectional = True\n",
    "\n",
    "seq_length = 30\n",
    "downsample = 6\n",
    "in_out_ratio = 5\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = get_encoder(encoder_feature_size,\n",
    "                      device,\n",
    "                      hidden_size=hidden_size,\n",
    "                      bidirectional=bidirectional)\n",
    "\n",
    "decoder = get_attn_decoder(decoder_feature_size,\n",
    "                           attention,\n",
    "                           device,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional_encoder=bidirectional)\n",
    "\n",
    "    \n",
    "decoder.batch_size = 1\n",
    "decoder.attention.batch_size = 1\n",
    "\n",
    "PATH = \"/home/jackg7/VT-Natural-Motion-Processing/models/set-2/model.pt\"\n",
    "\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "models = (encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the inference function\n",
    "\n",
    "Our inference function needs to take in a batch of input data, pass it through both the encoder and decoder, and then return the output.\n",
    "\n",
    "Note that this function is very similar to the *loss_batch* function defined in *src/seq2seq/training_utils.py*. Here it returns the outputs instead of the loss for training/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(data, models, device, use_attention=False, norm_quaternions=False):\n",
    "    encoder, decoder = models\n",
    "    input_batch, target_batch = data\n",
    "\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    seq_length = target_batch.shape[1]\n",
    "\n",
    "    input = input_batch.permute(1, 0, 2)\n",
    "    encoder_outputs, encoder_hidden = encoder(input)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.ones_like(target_batch[:, 0, :]).unsqueeze(0)\n",
    "    \n",
    "    outputs = torch.zeros_like(target_batch)\n",
    "\n",
    "    for t in range(seq_length):\n",
    "\n",
    "        if use_attention:\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "        target = target_batch[:, t, :].unsqueeze(0).double()\n",
    "            \n",
    "        output = decoder_output\n",
    "\n",
    "        if norm_quaternions:\n",
    "            original_shape = output.shape\n",
    "\n",
    "            output = output.view(-1,4)\n",
    "            output = F.normalize(output, p=2, dim=1).view(original_shape)\n",
    "\n",
    "        outputs[:, t, :] = output\n",
    "        \n",
    "        decoder_input = output.detach()\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling some data and running it through the inference function\n",
    "\n",
    "We can now sample some data and use our model to perform inference.\n",
    "\n",
    "We only trained the model to predict a single posture and did not use much training data as this is just a tutorial.\n",
    "\n",
    "Note we used normOrientations as output, so we have to put the orientation back into the original frame using the root's (pelvis) orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15330\n",
    "\n",
    "inp = torch.cat((orientationInputs[i:i+seq_length:downsample, :], accelerationInputs[i:i+seq_length:downsample, :]), dim=1)\n",
    "inp = inp.sub(mean).div(std_dev).double()\n",
    "\n",
    "out = fullBodyOrientations[i:i+seq_length:downsample, :].double()\n",
    "out = out[-1,:].unsqueeze(0) # trained our model to predict only a single pose\n",
    "\n",
    "data = (inp.unsqueeze(0), out.unsqueeze(0))\n",
    "      \n",
    "output = inference(data, models, device, use_attention=True, norm_quaternions=True)\n",
    "\n",
    "full_body = fullBodyOrientations[i:i+seq_length:downsample,:].clone()\n",
    "full_body = full_body[-1, :].unsqueeze(0)\n",
    "\n",
    "seq2seq_body = output.clone().squeeze(0)\n",
    "\n",
    "root_motion = root[i:i+seq_length:downsample, :].clone()\n",
    "root_motion = root_motion[-1, :].unsqueeze(0)\n",
    "\n",
    "root_motion = root_motion.unsqueeze(1).repeat(1, full_body.shape[1]//4, 1)\n",
    "\n",
    "full_body = quat_mul(root_motion, full_body.view(-1, full_body.shape[1]//4, 4)).view(full_body.shape)\n",
    "seq2seq_body = quat_mul(root_motion, seq2seq_body.view(-1, seq2seq_body.shape[1]//4, 4)).view(seq2seq_body.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ground truth posture and our seq2seq output, we can compare the motion using the Skeleton. \n",
    "\n",
    "The output won't look that great because we only used a single participant and a single epoch to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skeleton = Skeleton()\n",
    "bodies = torch.cat((full_body, seq2seq_body), dim=0).float()\n",
    "skeleton.compare_motion(bodies, azim=0, elev=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
